boot.ci(bootResults, type="bca", index=1)
junk.bca
junk$bca
junk$bca[4:5,]
junk$bca[4:5]
s.e. <- sqrt(-diag(solve(result$hessian)))
s.e. <- sqrt(-diag(solve(optim_result$hessian)))
s.e.
optim_result$hessian
solve(optim_result$hessian)
diag(solve(optim_result$hessian))
s.e. <- sqrt(diag(solve(optim_result$hessian)))
s.e.
glm_result <- glm(binomY ~ X1 + X2, data = glmData, family = binomial)
glm_result
optim_result
?glm
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
summary(modellm)
lm.beta(modellm)
coeftest(modellm)
?coeftest
bootReg <- function(formula, data, indices){
d<- data[indices,]
fit <- lm(formula, data = d)
return(coef(fit))
}
# Use bootstrap to estimate the model, use 2000 samples as recommended in the text
bootResults <- boot(statistic = bootReg,
formula =  predictedVal ~ X1 + X2, data = predictorVar, R = 2000)
# Summarize the results of bootstrap
bootResults
ci_1 <- boot.ci(bootResults, type="bca", index=1)
ci_1$bca[4:5]
ci_2 <- boot.ci(bootResults, type="bca", index=2)
ci_2$bca[4:5]
ci_3 <- boot.ci(bootResults, type="bca", index=3)
ci_3$bca[4:5]
# Look at confidence intervals from the LM model
confint(modellm)
coeftest(modellm, vcov = vcovHC)
vcovHC(modellm, type = "HC")
# Extract the R^2 values
sandwich_se <- diag(vcovHC(modellm, type = "HC"))^0.5
sandwich_se
# Generate data
# Assume that data from part 1 of this assignment form observations from the world
# Generate Y = 1 when predictedVal is positive and Y = 0 when predictedVal is negative
binomY <- ifelse(predictedVal < 0, 0, 1)
X
predictorVar <- X
# Generate the complete data for Part 2 of this assignment, call it dataDF
dataDF <- cbind(binomY, predictedVal, predictorVar)
dataDF
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)
Y
X
X <- cbind(1, X)
X
X$X3 <- Null # remove the X3 column, we are not using it
X
X <- predictorVar
X
X$X3 <- Null
X$X3
X$X3 <- NULL
X
rm(list=ls())
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
rm(list=ls())
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
```
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Next, fit a linear model to the data
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
rm(list=ls())
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
summary(modellm)
lm.beta(modellm)
coeftest(modellm)
# Set up the Bootstrap function
# Mimic code from the textbook
bootReg <- function(formula, data, indices){
d<- data[indices,]
fit <- lm(formula, data = d)
return(coef(fit))
}
# Use bootstrap to estimate the model, use 2000 samples as recommended in the text
bootResults <- boot(statistic = bootReg,
formula =  predictedVal ~ X1 + X2, data = predictorVar, R = 2000)
# Summarize the results of bootstrap
bootResults
# Obtain bootstrap confidence levels for the Intercept and the 2 other Coefficients
ci_1 <- boot.ci(bootResults, type="bca", index=1)
ci_1$bca[4:5]
ci_2 <- boot.ci(bootResults, type="bca", index=2)
ci_2$bca[4:5]
ci_3 <- boot.ci(bootResults, type="bca", index=3)
ci_3$bca[4:5]
# Look at confidence intervals from the LM model
confint(modellm)
coeftest(modellm, vcov = vcovHC)
vcovHC(modellm, type = "HC")
sandwich_se <- diag(vcovHC(modellm, type = "HC"))^0.5
sandwich_se
# Generate data
# Assume that data from part 1 of this assignment form observations from the world
# Generate Y = 1 when predictedVal is positive and Y = 0 when predictedVal is negative
binomY <- ifelse(predictedVal < 0, 0, 1)
# Restore predictorVar from an earlier version of predictors
# We need a clean set up so that matrix multiplication can be done
predictorVar <- X
# Generate the complete data for Part 2 of this assignment, call it dataDF
dataDF <- cbind(binomY, predictedVal, predictorVar)
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)
predictorVar
Y
X
b=as.matrix(c(b0, b1, b2))
X <- cbind(1, X) # get the constant term factored in
Y
X
b
logit(Y, X, b)
optim_result <- optim(b,fn=logit,y=Y,x=X, method='BFGS',hessian=TRUE)
optim_result$convergence
optim_result$par
solve(optim_result$hessian)
s.e. <- sqrt(diag(solve(optim_result$hessian)))
s.e.
glmData <- cbind(binomY, predictorVar)
glm_result <- glm(binomY ~ X1 + X2, data = glmData, family = binomial)
summary(glm_result)
optim_result
getwd()
setwd("../Homework_15_203")
getwd()
library(car)
library(mlogit)
library(PASWR)
? titanic3
summary(titanic3)
library(ggplot)
library(dplyr)
# You will run a logistic regression to predict the probability of survival
# (survived).  Begin with one predictor variable, parch, representing the
# number of parents and children that each individual had on the boat.
#
# Create a plot of the actual survival percentages by parch. Comment on
# whether the logistic model seems appropriate---hint: is it something
# that has a logistic shape (flattened-s shaped) or can be part of that
# shape, or not?
#
# Run the logistic model and comment on both the statistical and
# practical significance of your predictor. Add other independent
# variables that you think make sense.
#
# Use "predict" to predict probabilities for changes in one of
# your independent variables.
#
# for example, if you have estimated "model1," you can predict
# probabilities out of your model by doing this:
#
#   mydata  = data.frame( parch = 0:10 )
# mydata$predicted.probability = predict( model1, newdata = mydata, type='response' )
#
#
# Continue exploring the dataset and find something interesting to share with the class!
# You may want to include parch^2 as well. Remember that you need to use I(parch^2) on
# the right hand side to prevent glm from trying to interpret your parch^2  in some other way.
#
#
# - Use the anova command to compare the two models.
# - Plot predicted probabilities against the actual observation of survival rate.
DT <- titanic3
names(DT)
SR = DT %>% group_by(parch) %>%
summarize(survival.rate = mean(survived, na.rm=TRUE), count=n())
SR
ggplot(SR, aes(parch, survival.rate, size=count)) +
geom_line(size=0.7) +
geom_point(color='orange')
model_titanic1 <- glm(survived ~ parch, data = titanic3, family = binomial())
summary(model_titanic1)
exp(coef(model_titanic1))
model_titanic2 <- glm(survived ~ parch + I(parch^2), data = titanic3, family = binomial())
summary(model_titanic2)
exp(coef(model_titanic2))
getwd90
GETWD()
getwd()
datingDF <- read.csv("Dating.csv", header=TRUE)
datingDF
head(datingDF)
nrow(datingDF)
datingDF <- read.csv("Dating.csv")
head(datingDF)
nrow(datingDF)
names(datingDF)
str(datingDF$marital_status)
str(datingDF$use_reddit)
class(datingDF$marital_status)
class(datingDF$use_reddit)
levels(datingDF$marital_status)
levsl(datingDF$use_reddit)
levels(datingDF$use_reddit)
setwd("../Homework_12_203")
library(gmodels)
library(ggplot2)
library(car)
library(lsr)
load("GSS.Rdata")
head(GSS)
levels(GSS$politics)
levels(GSS$news)
str(datingDF$region)
str(datingDF$life_quality)
class(datingDF$region)
class(datingDF$life_quality)
levels(datingDF$region)
levels(datingDF$life_quality)
class(GSS$age)
class(GSS$country)
levels(GSS$country)
class(datingDF$flirted_online)
levels(datingDF$flirted_online)
class(datingDF$years_in_relationship)
levels(datingDF$years_in_relationship)
str(datingDF$flirted_online)
str(datingDF$years_in_relationship)
class(datingDF$flirted_online)
class(datingDF$years_in_relationship)
levels(datingDF$flirted_online)
levels(datingDF$years_in_relationship)
str(datingDF$lgbt)
str(datingDF$adults_in_household)
class(datingDF$lgbt)
class(datingDF$adults_in_household)
levels(datingDF$lgbt)
levels(datingDF$adults_in_household)
levels(GSS$income91)
levels(GSS$visitart)
str(datingDF$flirted_online)
str(datingDF$years_in_relationship)
class(datingDF$flirted_online)
class(datingDF$years_in_relationship)
levels(datingDF$flirted_online)
levels(datingDF$years_in_relationship)
str(datingDF$lgbt)
str(datingDF$adults_in_household)
class(datingDF$lgbt)
class(datingDF$adults_in_household)
levels(datingDF$lgbt)
levels(datingDF$adults_in_household)
head(datingDF)
nrow(datingDF)
nrow(complete.cases(datingDF$children0_5, datingDF$children6_11, datingDF$children12_17))
?complete.cases
length(complete.cases(datingDF$children0_5, datingDF$children6_11, datingDF$children12_17))
levels(datingDF$life_quality)
junk <- datingDF[datingDF$life_quality != "Don't know" & datingDF$life_quality != "Refused",]
head(junk)
summary(junk$life_quality'')
summary(junk$life_quality)
junk$life_quality = droplevels(junk$life_quality)
summary(junk$life_quality)
head(junk)
levels(junk$life_quality)
junk$new_life_quality <- as.numeric(junk$life_quality)
head(junk)
range(junk$new_life_quality)
range(-junk$new_life_quality)
rank(-junk$new_life_quality)
max(junk$new_life_quality)
min(junk$new_life_quality)
junk$new_new_life_quality <- (max(junk$new_life_quality) + min(junk$new_life_quality) - junk$new_life_quality)
head(junk)
mean(unk$new_new_life_quality)
mean(junk$new_new_life_quality)
junk$years_in_relationship[1:100,]
head(junk$years_in_relationship)
class(junk$years_in_relationship)
as.numeric(junk$years_in_relationship)
head(as.numeric(as.character(junk$years_in_relationship)))
as.character(junk$years_in_relationship)
as.numeric(as.character(junk$years_in_relationship))
junk <- junk[junk$years_in_relationship ! = " ",]
junk <- junk[junk$years_in_relationship != " ",]
nrow(junk)
as.numeric(as.character(junk$years_in_relationship))
as.character(junk$years_in_relationship)
junk <- junk[junk$years_in_relationship != " " & junk$years_in_relationship != "Refused",]
as.numeric(as.character(junk$years_in_relationship))
junk$yir_numer <- as.numeric(as.character(junk$years_in_relationship))
junkjunk <- complete.cases(junk$years_in_relationship, junk$life_quality, junk$use_internet)
nrow(junkjunk)
complete.cases(junk$years_in_relationship, junk$life_quality, junk$use_internet)
junk <- junk[complete.cases(junk$years_in_relationship, junk$life_quality, junk$use_internet),]
nrow(junk)
junk <- junk[complete.cases(junk$yir_num, junk$life_quality, junk$use_internet),]
nrow(junk)
junk$life_quality
datingDF$use_internet
levels(junk$use_internet)

library(stats)
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting for now
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar[,2]
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N,0,hskdTrigger(x)))
# Implement the linear predictor equation
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
# Fit a linear model to the data
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
# Look at the data from the lm model
summary(modellm)
coeftest(modellm)
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
# Set up the Bootstrap function
# Mimic code from the textbook
bootReg <- function(formula, data, indices){
d<- data[indices,]
fit <- lm(formula, data = d)
return(coef(fit))
}
# Use bootstrap to estimate the model, use 2000 samples as recommended in the text
bootResults <- boot(statistic = bootReg,
formula =  predictedVal ~ X1 + X2, data = predictorVar, R = 2000)
# Summarize the results of bootstrap
summary(bootResults)
bootResults
# Obtain bootstrap confidence levels for the Intercept and the 2 other Coefficients
boot.ci(bootResults, type="bca", index=1)
boot.ci(bootResults, type="bca", index=2)
boot.ci(bootResults, type="bca", index=3)
coeftest(modellm, vcov = vcovHC)
vcovHC(modellm, type = "HC")
sandwich_se <- diag(vcovHC(modellm, type = "HC"))^0.5
sandwich_se
# Generate data
# Assume that data from part 1 of this assignment form observations from the world
# Generate Y = 1 when predictedVal is positive and Y = 0 when predictedVal is negative
binomY <- ifelse(predictedVal < 0, 0, 1)
# Restore predictorVar from an earlier version of predictors
# We need a clean set up so that matrix multiplication can be done
predictorVar <- X
# Generate the complete data for Part 2 of this assignment, call it dataDF
dataDF <- cbind(binomY, predictedVal, predictorVar)
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
b=as.matrix(c(0, 0. 0))
b=as.matrix(c(0, 0, 0))
# Call the logit function with Y, x and b
logit(as.matrix(binomY), as.matrix(predictorVar), as.matrix(b))
# Perform the numerical optimization using the optim() function
optim_result <- optim(b,fn=logit,y=as.matrix(binomY),x=as.matrix(predictorVar), method='BFGS',hessian=TRUE)
# Check convergence to make sure that optim() did converge
optim_result$convergence
# Check the parameters, report the b values
optim_result$par
# Per handout notes, get the standard error for each parameter
solve(optim_result$hessian)
# Compare with glm
glmData <- cbind(binomY, predictorVar)
glm_result <- glm(binomY ~ X1 + X2 + X3, data = glmData, family = binomial)
glm_result
summary(glm_result)
?rnorm
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return((sum_loglh))
}
logit(as.matrix(binomY), as.matrix(predictorVar), as.matrix(b))
optim_result <- optim(b,fn=logit,y=as.matrix(binomY),x=as.matrix(predictorVar), method='BFGS',hessian=TRUE)
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert the coefficients into a matrix as well
b=as.matrix(c(b0, b1, b2))
# Call the logit function with Y, x and b
logit(as.matrix(binomY), as.matrix(predictorVar), as.matrix(b))
# Perform the numerical optimization using the optim() function
optim_result <- optim(b,fn=logit,y=as.matrix(binomY),x=as.matrix(predictorVar), method='BFGS',hessian=TRUE)
optim_result
optim_result$par
logit(as.matrix(binomY), as.matrix(predictorVar), as.matrix(b))
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return((sum_loglh))
}
# Convert the coefficients into a matrix as well
b=as.matrix(c(b0, b1, b2))
# Call the logit function with Y, x and b
logit(as.matrix(binomY), as.matrix(predictorVar), as.matrix(b))
optim_result <- optim(b,fn=logit,y=as.matrix(binomY),x=as.matrix(predictorVar), method='BFGS',hessian=TRUE)
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)
# Convert the coefficients into a matrix as well
b=as.matrix(c(b0, b1, b2))
# Call the logit function with Y, x and b
logit(Y, x, b)
# Perform the numerical optimization using the optim() function
optim_result <- optim(b,fn=logit,y=Y,x=X, method='BFGS',hessian=TRUE)
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)
# Convert the coefficients into a matrix as well
b=as.matrix(c(b0, b1, b2))
# Call the logit function with Y, x and b
logit(Y, X, b)
# Perform the numerical optimization using the optim() function
optim_result <- optim(b,fn=logit,y=Y,x=X, method='BFGS',hessian=TRUE)
optim_result
?coeftest
anova(modellm, bootResults)
lm.beta(modellm)
confint(modellm)
library(QuantPsyc)
suppressMessages(library(QuantPsyc))
lm.beta(modellm)
confint(modellm)
summary(boot.ci(bootResults, type="bca", index=1))
junk <- boot.ci(bootResults, type="bca", index=1)
str(junk)
boot.ci(bootResults, type="bca", index=1)
junk.bca
junk$bca
junk$bca[4:5,]
junk$bca[4:5]
s.e. <- sqrt(-diag(solve(result$hessian)))
s.e. <- sqrt(-diag(solve(optim_result$hessian)))
s.e.
optim_result$hessian
solve(optim_result$hessian)
diag(solve(optim_result$hessian))
s.e. <- sqrt(diag(solve(optim_result$hessian)))
s.e.
glm_result <- glm(binomY ~ X1 + X2, data = glmData, family = binomial)
glm_result
optim_result
?glm
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
summary(modellm)
lm.beta(modellm)
coeftest(modellm)
?coeftest
bootReg <- function(formula, data, indices){
d<- data[indices,]
fit <- lm(formula, data = d)
return(coef(fit))
}
# Use bootstrap to estimate the model, use 2000 samples as recommended in the text
bootResults <- boot(statistic = bootReg,
formula =  predictedVal ~ X1 + X2, data = predictorVar, R = 2000)
# Summarize the results of bootstrap
bootResults
ci_1 <- boot.ci(bootResults, type="bca", index=1)
ci_1$bca[4:5]
ci_2 <- boot.ci(bootResults, type="bca", index=2)
ci_2$bca[4:5]
ci_3 <- boot.ci(bootResults, type="bca", index=3)
ci_3$bca[4:5]
# Look at confidence intervals from the LM model
confint(modellm)
coeftest(modellm, vcov = vcovHC)
vcovHC(modellm, type = "HC")
# Extract the R^2 values
sandwich_se <- diag(vcovHC(modellm, type = "HC"))^0.5
sandwich_se
# Generate data
# Assume that data from part 1 of this assignment form observations from the world
# Generate Y = 1 when predictedVal is positive and Y = 0 when predictedVal is negative
binomY <- ifelse(predictedVal < 0, 0, 1)
X
predictorVar <- X
# Generate the complete data for Part 2 of this assignment, call it dataDF
dataDF <- cbind(binomY, predictedVal, predictorVar)
dataDF
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)
Y
X
X <- cbind(1, X)
X
X$X3 <- Null # remove the X3 column, we are not using it
X
X <- predictorVar
X
X$X3 <- Null
X$X3
X$X3 <- NULL
X
rm(list=ls())
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
rm(list=ls())
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
```
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Next, fit a linear model to the data
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
rm(list=ls())
library(MASS)
suppressMessages(library(lmtest))
library(gvlma)
suppressMessages(library(boot))
library(sandwich)
library(stats)
suppressMessages(library(QuantPsyc))
# Set a seed for repeatable results, when needed
set.seed(123456)
# Select number of data points, a general setting
N <- 100
# Per homework handout:
# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)
# Per the homework handout:
# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))
# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")
# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5
# The function that helps create heteroskedasticity
hskdTrigger <- function(x) {
# use one of the predictor variables
# Reference: StackForge
1.0 + 0.4*predictorVar$X2
}
# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values.
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))
# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon
# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it
# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)
# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)
# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)
# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
summary(modellm)
lm.beta(modellm)
coeftest(modellm)
# Set up the Bootstrap function
# Mimic code from the textbook
bootReg <- function(formula, data, indices){
d<- data[indices,]
fit <- lm(formula, data = d)
return(coef(fit))
}
# Use bootstrap to estimate the model, use 2000 samples as recommended in the text
bootResults <- boot(statistic = bootReg,
formula =  predictedVal ~ X1 + X2, data = predictorVar, R = 2000)
# Summarize the results of bootstrap
bootResults
# Obtain bootstrap confidence levels for the Intercept and the 2 other Coefficients
ci_1 <- boot.ci(bootResults, type="bca", index=1)
ci_1$bca[4:5]
ci_2 <- boot.ci(bootResults, type="bca", index=2)
ci_2$bca[4:5]
ci_3 <- boot.ci(bootResults, type="bca", index=3)
ci_3$bca[4:5]
# Look at confidence intervals from the LM model
confint(modellm)
coeftest(modellm, vcov = vcovHC)
vcovHC(modellm, type = "HC")
sandwich_se <- diag(vcovHC(modellm, type = "HC"))^0.5
sandwich_se
# Generate data
# Assume that data from part 1 of this assignment form observations from the world
# Generate Y = 1 when predictedVal is positive and Y = 0 when predictedVal is negative
binomY <- ifelse(predictedVal < 0, 0, 1)
# Restore predictorVar from an earlier version of predictors
# We need a clean set up so that matrix multiplication can be done
predictorVar <- X
# Generate the complete data for Part 2 of this assignment, call it dataDF
dataDF <- cbind(binomY, predictedVal, predictorVar)
logit <- function(y, x, b)
{
sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
return(abs(sum_loglh))
}
# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)
predictorVar
Y
X
b=as.matrix(c(b0, b1, b2))
X <- cbind(1, X) # get the constant term factored in
Y
X
b
logit(Y, X, b)
optim_result <- optim(b,fn=logit,y=Y,x=X, method='BFGS',hessian=TRUE)
optim_result$convergence
optim_result$par
solve(optim_result$hessian)
s.e. <- sqrt(diag(solve(optim_result$hessian)))
s.e.
glmData <- cbind(binomY, predictorVar)
glm_result <- glm(binomY ~ X1 + X2, data = glmData, family = binomial)
summary(glm_result)
optim_result

---
title: "W203_Homework_14_I"
author: "Natarajan Shankar"
date: "August 9, 2016"
output: pdf_document
---

```{r error=TRUE, fig.width = 6, fig.height = 4.5}
library(MASS); library(gvlma); library(sandwich) ; library(stats)
suppressMessages(library(lmtest)); suppressMessages(library(boot))
suppressMessages(library(QuantPsyc))

# Set a seed for repeatable results, when needed
set.seed(123456)

# Select number of data points, a general setting
N <- 100

# Creating independent variables
# first, the covariance matrix for our multi-variate normal
# This determines the correlation of our independent variables
sig <- matrix(c(2, .5, .25, .5, 1, 0, .25, 0, 1), nrow=3)

# Now the variables:
predictorVar = as.data.frame(mvrnorm(n = N, mu = rep(1,3), Sigma = sig))

# For convenience, name the predictor variables
# THe third will not be used in this assignment but leave it in there
colnames(predictorVar) <- c("X1", "X2", "X3")

# Set the coefficients as per the homework handout
b0 = 1 ; b1 = 2 ; b2 = -5

# The function that helps create heteroskedasticity
hskdTrigger <- function(x) { 
  # use one of the predictor variables
  # Reference: StackForge
  1.0 + 0.4*predictorVar$X2
}

# Use the function that helps create heteroskedasticity to generate data
# Suppress warnings that call out presence of NaN values. 
epsilon = suppressWarnings(rnorm(N, 0, hskdTrigger(x)))

# Implement the linear predictor equation, use the error term
predictedVal = b0 + b1*predictorVar$X1 + b2 * predictorVar$X2 + epsilon

# Save an X dataframe for part 2 of this asignment
X <- predictorVar
X$X3 <- NULL # remove the X3 column, we will not be using it

# For running models, keep the Y data together with the predictor data
predictorVar <- cbind(predictedVal, predictorVar)

# Ensure that the generated graph indicates heteroskedasticity
# Plot residuals against ptedicted values
plot(predictedVal, epsilon)

# Using a Least Squares fit, draw a reference line so that a
# general orientation of slope is available on the plot
abline(lsfit(predictedVal, epsilon), col = "red")
```
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Next, fit a linear model to the data
modellm <- lm(predictedVal ~ X1 + X2 , data = predictorVar)

# test for Heterskedsticity by running  the Breusch Pagan test
bpresults <- bptest(modellm)
bpresults
```
## The Breusch Pagan test confirms that based upon the statistical confidence of ```r signif(bpresults$p.value, digits=6)```, the Null hypothesis of Homoskedasticity can be rejected

```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Look at the data from the lm model
summary(modellm)
```
# The lm model fits the data very well as shown by the adjusted R-squared value. 90% of the Response variable can be explained by the results of the LM model
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Check confidence intervals of the coefficients
# Get the standard beta values
lm.beta(modellm)
```
# Coefficient of X1 contributes in positive manner to the Response variable, The coefficient of X2 contributes in negative manner to the response variable

```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Perform a coeftest to get the robust standard errors
coeftest(modellm)
```

```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Set up the Bootstrap function
# Mimic code from the textbook
bootReg <- function(formula, data, indices){
  d<- data[indices,]
  fit <- lm(formula, data = d)
  return(coef(fit))
}

# Use bootstrap to estimate the model, use 2000 samples as recommended in the text
bootResults <- boot(statistic = bootReg, 
              formula =  predictedVal ~ X1 + X2, data = predictorVar, R = 2000)
# Summarize the results of bootstrap
bootResults
```
## Bootstrap: Coeffficients of approximately 1.xx, 2.xx and -5.xx are almost the same as in the LM model. 
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Obtain bootstrap confidence levels for the Intercept and the 2 other Coefficients
ci_1 <- boot.ci(bootResults, type="bca", index=1)
ci_1$bca[4:5]
ci_2 <- boot.ci(bootResults, type="bca", index=2)
ci_2$bca[4:5]
ci_3 <- boot.ci(bootResults, type="bca", index=3)
ci_3$bca[4:5]

# Look at confidence intervals from the LM model
confint(modellm)
```
## Bootstrap: Confidence levels seen with Bootstrap are close in value to those seen with the LM model. The data distributional requirements are well compensated for by the bootstrap method
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Run the coeftest test to get Robust Standard Errors
coeftest(modellm, vcov = vcovHC)
```
## Sandwich/Coeftest: Coeffficients b0, b1, b2 are associated with a very high level of statistical significance
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
vcovHC(modellm, type = "HC")
```
## The heteroskedasticity consistent covariance matrix shows very low covariance amongst the coefficients
```{r error=TRUE, fig.width = 6, fig.height = 4.5}
# Extract the R^2 values
sandwich_se <- diag(vcovHC(modellm, type = "HC"))^0.5
sandwich_se
```
## The LM model shows a high correlation towards the Response variable
\pagebreak

# Part 2

```{r error=TRUE, fig.width = 6, fig.height = 4.5}

# Generate data
# Assume that data from part 1 of this assignment form observations from the world
# Generate Y = 1 when predictedVal is positive and Y = 0 when predictedVal is negative
binomY <- ifelse(predictedVal < 0, 0, 1)

# Restore predictorVar from an earlier version of predictors
# We need a clean set up so that matrix multiplication can be done
predictorVar <- X

# Generate the complete data for Part 2 of this assignment, call it dataDF
dataDF <- cbind(binomY, predictedVal, predictorVar)

```
# What is the likelihood that y_i is 1?
##  $$ p^{y_i} * (1-p)^{(1-y_i)} $$ and when $$ y_i = 1 $$ the likelihood that y_i = 1 is $$ p $$

# What is the likelihood that y_i is 0?
## $$ p^{y_i} * (1-p)^{(1-y_i)} $$ and when $$ y_i = 0$$ the likelihood that y_i = 0 is $$ (1-p) $$


# What is the sum of log likelihoods of observing all of y
##     $$ LL = log[p^y_i * (1-p)^{(1-y_i)}] $$
##     $$ LL = log[p^y_i] + log[(1-p)^{(1-y_i)}] $$
##     $$ LL = y_i * log[p] + (1-y_i) * log[1-p] $$
##     now, given that $$ p = \frac{1}{1+ exp(-X * b)} $$
##     $$ LL = -y_i * log[1+ exp(-X * b)] - (1-y_i) * log[1+ exp(-X * b)] $$
## The above equation is implemented in logit below


```{r error=TRUE, fig.width = 6, fig.height = 4.5}

logit <- function(y, x, b)
{
  sum_loglh <- sum(-y*log(1 + exp(-(x%*%b))) - (1-y)*log(1 + exp(x%*%b)))
  return(abs(sum_loglh))
}

# Convert Y and X into matrices
Y <- as.matrix(binomY)
X <- as.matrix(predictorVar)

# Convert the coefficients into a matrix as well
b=as.matrix(c(b0, b1, b2))
X <- cbind(1, X) # get the constant term factored in


# Call the logit function with Y, x and b
logit(Y, X, b)

# Perform the numerical optimization using the optim() function
optim_result <- optim(b,fn=logit,y=Y,x=X, method='BFGS',hessian=TRUE)

# Check convergence to make sure that optim() did converge
optim_result$convergence

# Check the parameters, report the b values
optim_result$par

# Per handout notes, get the standard error for each parameter
solve(optim_result$hessian)

# Look for standard error of each parameter
s.e. <- sqrt(diag(solve(optim_result$hessian)))
s.e.

# Compare with glm
glmData <- cbind(binomY, predictorVar)
glm_result <- glm(binomY ~ X1 + X2, data = glmData, family = binomial)
summary(glm_result)
```

# Results from glm show coeffoicients that match the match the coefficients derived via optim()